{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "b64roadsigns.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "EAQoosm5LEb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dotscience Roadsigns Demo"
      ]
    },
    {
      "metadata": {
        "id": "9lqdQLFrLEb9",
        "colab_type": "code",
        "outputId": "3792dc60-34ef-4cf5-e2e9-0ff1c08f6a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dotscience\n",
        "import dotscience as ds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import skimage.morphology as morp\n",
        "from skimage.filters import rank\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dotscience in /usr/local/lib/python3.6/dist-packages (0.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nwcjv74qLEcA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds.interactive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6-u3iChLEcD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds.input(\"train.p\")\n",
        "ds.input(\"valid.p\")\n",
        "ds.input(\"test.p\")\n",
        "train = pickle.load(open(\"train.p\",\"rb\"))\n",
        "valid = pickle.load(open(\"valid.p\",\"rb\"))\n",
        "test = pickle.load(open(\"test.p\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHDK8qpJLEcF",
        "colab_type": "code",
        "outputId": "6504df24-3fee-493f-dd48-e4db42e0c604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Mapping ClassID to traffic sign names\n",
        "signs = []\n",
        "with open('signnames.csv', 'r') as csvfile:\n",
        "    signnames = csv.reader(csvfile, delimiter=',')\n",
        "    next(signnames,None)\n",
        "    for row in signnames:\n",
        "        signs.append(row[1])\n",
        "    csvfile.close()\n",
        "    \n",
        "ds.input(\"signnames.csv\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'signnames.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "R18_yOSBPZj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "xPWhg61hPZv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2214ca8f-c07f-4de6-ec65-92a466a9b3d6"
      },
      "cell_type": "code",
      "source": [
        "!mv ../train.p ."
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat '../train.p': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "55WeTtuPPsxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f3d67b1-ba69-44b4-e3a1-da46b26d7690"
      },
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1PaeZQvnLEcJ",
        "colab_type": "code",
        "outputId": "c8d063dc-8531-4d69-da2b-4168b67258f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "X_train, y_train = train['features'], train['labels']\n",
        "X_valid, y_valid = valid['features'], valid['labels']\n",
        "X_test, y_test = test['features'], test['labels']\n",
        "\n",
        "# Number of training examples\n",
        "n_train = X_train.shape[0]\n",
        "\n",
        "# Number of testing examples\n",
        "n_test = X_test.shape[0]\n",
        "\n",
        "# Number of validation examples.\n",
        "n_validation = X_valid.shape[0]\n",
        "\n",
        "# What's the shape of an traffic sign image?\n",
        "image_shape = X_train[0].shape\n",
        "\n",
        "# How many unique classes/labels there are in the dataset.\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "print(\"Number of training examples: \", n_train)\n",
        "print(\"Number of testing examples: \", n_test)\n",
        "print(\"Number of validation examples: \", n_validation)\n",
        "print(\"Image data shape =\", image_shape)\n",
        "print(\"Number of classes =\", n_classes)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples:  34799\n",
            "Number of testing examples:  12630\n",
            "Number of validation examples:  4410\n",
            "Image data shape = (32, 32, 3)\n",
            "Number of classes = 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W8LLewhLLEcL",
        "colab_type": "code",
        "outputId": "5260c4af-4901-48fc-e6ad-0ec291e410ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "HA_hpKhSLEcO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "metadata": {
        "id": "g0WXq0oHLEcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define helper functions\n",
        "def list_images(dataset, dataset_y, ylabel=\"\", cmap=None):\n",
        "    \"\"\"\n",
        "    Display a list of images in a single figure with matplotlib.\n",
        "        Parameters:\n",
        "            images: An np.array compatible with plt.imshow.\n",
        "            lanel (Default = No label): A string to be used as a label for each image.\n",
        "            cmap (Default = None): Used to display gray images.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 16))\n",
        "    for i in range(6):\n",
        "        plt.subplot(1, 6, i+1)\n",
        "        indx = random.randint(0, len(dataset))\n",
        "        #Use gray scale color map if there is only one channel\n",
        "        cmap = 'gray' if len(dataset[indx].shape) == 2 else cmap\n",
        "        plt.imshow(dataset[indx], cmap = cmap)\n",
        "        plt.xlabel(signs[dataset_y[indx]])\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
        "    plt.show()\n",
        "    \n",
        "def histogram_plot(dataset, label):\n",
        "    \"\"\"\n",
        "    Plots a histogram of the input data.\n",
        "        Parameters:\n",
        "            dataset: Input data to be plotted as a histogram.\n",
        "            lanel: A string to be used as a label for the histogram.\n",
        "    \"\"\"\n",
        "    hist, bins = np.histogram(dataset, bins=n_classes)\n",
        "    width = 0.7 * (bins[1] - bins[0])\n",
        "    center = (bins[:-1] + bins[1:]) / 2\n",
        "    plt.bar(center, hist, align='center', width=width)\n",
        "    plt.xlabel(label)\n",
        "    plt.ylabel(\"Image count\")\n",
        "    plt.show()\n",
        "    \n",
        "def gray_scale(image):\n",
        "    \"\"\"\n",
        "    Convert images to gray scale.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "def local_histo_equalize(image):\n",
        "    \"\"\"\n",
        "    Apply local histogram equalization to grayscale images.\n",
        "        Parameters:\n",
        "            image: A grayscale image.\n",
        "    \"\"\"\n",
        "    kernel = morp.disk(30)\n",
        "    img_local = rank.equalize(image, selem=kernel)\n",
        "    return img_local\n",
        "\n",
        "def image_normalize(image):\n",
        "    \"\"\"\n",
        "    Normalize images to [0, 1] scale.\n",
        "        Parameters:\n",
        "            image: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    image = np.divide(image, 255)\n",
        "    return image\n",
        "\n",
        "def preprocess(data):\n",
        "    \"\"\"\n",
        "    Applying the preprocessing steps to the input data.\n",
        "        Parameters:\n",
        "            data: An np.array compatible with plt.imshow.\n",
        "    \"\"\"\n",
        "    gray_images = list(map(gray_scale, data))\n",
        "    equalized_images = list(map(local_histo_equalize, gray_images))\n",
        "    n_training = data.shape\n",
        "    normalized_images = np.zeros((n_training[0], n_training[1], n_training[2]))\n",
        "    for i, img in enumerate(equalized_images):\n",
        "        normalized_images[i] = image_normalize(img)\n",
        "    normalized_images = normalized_images[..., None]\n",
        "    return normalized_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2TkMteOrLEcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_valid_preprocessed = preprocess(X_valid)\n",
        "X_test_preprocessed = preprocess(X_test)\n",
        "X_train_preprocessed = preprocess(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQas32gfLEcW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train & test model"
      ]
    },
    {
      "metadata": {
        "id": "k4KM_teTNDqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder model\n",
        "\n",
        "Uses `tf.keras` to decode base64, and resize the image to a tensor of shape (32, 32, 1).\n",
        "\n",
        "Note that this model _must_ be supplied urlsafe base64. You can convert regular base64 to urlsafe using Python's [`base64` module](https://docs.python.org/3.7/library/base64.html)."
      ]
    },
    {
      "metadata": {
        "id": "M4UwQPXsNMmg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_and_decode(img_str):\n",
        "    #print(\"[preprocess_and_decode] got %s\" % (img_str,))\n",
        "    img = tf.io.decode_base64(img_str)\n",
        "    img = tf.image.decode_jpeg(img, channels=1)\n",
        "    img = tf.image.resize_images(img, (32, 32))\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    #img = preprocess(tf.Tensor([img]))\n",
        "    return img\n",
        "  \n",
        "InputLayer = tf.keras.Input(shape = (1,),dtype=\"string\")\n",
        "OutputLayer = tf.keras.layers.Lambda(lambda img : tf.map_fn(lambda im : preprocess_and_decode(im[0]), img, dtype=\"float32\"))(InputLayer)\n",
        "base64_model = tf.keras.Model(InputLayer, OutputLayer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "paiHkUv1NHUV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional neural net\n"
      ]
    },
    {
      "metadata": {
        "id": "MXYEnlKnLEcW",
        "colab_type": "code",
        "outputId": "396224c6-0c75-4697-bcf9-555eb6a73c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes=43\n",
        "conv = tf.keras.models.Sequential()\n",
        "conv.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(32, 32, 1)))\n",
        "conv.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "conv.add(tf.keras.layers.Conv2D(32, (5, 5), activation='relu'))\n",
        "conv.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "conv.add(tf.keras.layers.Flatten())\n",
        "conv.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
        "conv.add(tf.keras.layers.Dense(units = num_classes, activation='softmax'))\n",
        "\n",
        "conv.compile(optimizer=ds.parameter(\"optimizer\", 'adam'),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='acc')\n",
        "\n",
        "conv.fit(X_train_preprocessed, y_train,\n",
        "          epochs=ds.parameter(\"epochs\", 1),\n",
        "          verbose=1,\n",
        "          validation_data=(X_valid_preprocessed, y_valid),\n",
        "          callbacks=[es])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 34799 samples, validate on 4410 samples\n",
            "34799/34799 [==============================] - 74s 2ms/sample - loss: 0.6394 - acc: 0.8204 - val_loss: 0.3084 - val_acc: 0.9095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa50555e630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "bvrR5aacLEca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds.summary(\"accuracy\", conv.evaluate(X_test_preprocessed, y_test)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7WnqC7FOJaj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combined model of models\n",
        "\n",
        "We wire up the decoder and the convolutional neural net, so we can serve them both as a single model.\n"
      ]
    },
    {
      "metadata": {
        "id": "woc5FvEJONHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "base64_input = base64_model.input\n",
        "final_output = conv(base64_model.output)\n",
        "model = tf.keras.Model(base64_input,final_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kPMmvjKlLEcd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Save the `SavedModel` model to Dotscience\n"
      ]
    },
    {
      "metadata": {
        "id": "ZMXt3Ac9LEce",
        "colab_type": "code",
        "outputId": "46482c29-efc5-4c09-8cce-73aa60d8b6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# Fetch the Keras session and save the model\n",
        "# The signature definition is defined by the input and output tensors,\n",
        "# and stored with the default serving key\n",
        "import tempfile\n",
        "\n",
        "MODEL_DIR = \"./model\"\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "if os.path.isdir(export_path):\n",
        "  print('\\nAlready saved a model, cleaning up\\n')\n",
        "  !rm -r {export_path}\n",
        "\n",
        "tf.saved_model.simple_save(\n",
        "    tf.keras.backend.get_session(),\n",
        "    export_path,\n",
        "    inputs={'input_image_bytes': model.input}, \n",
        "    outputs={t.name:t for t in model.outputs})\n",
        "\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "export_path = ./model/1\n",
            "\n",
            "\n",
            "Already saved a model, cleaning up\n",
            "\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./model/1/saved_model.pb\n",
            "\n",
            "Saved model:\n",
            "total 26108\n",
            "-rw-r--r-- 1 root root 26728178 Apr 18 14:38 saved_model.pb\n",
            "drwxr-xr-x 2 root root     4096 Apr 18 14:38 variables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nI3HxuOWLEch",
        "colab_type": "code",
        "outputId": "ce4dfe85-71d4-4ef8-808f-b991970a4ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['input_image_bytes'] tensor_info:\n",
            "        dtype: DT_STRING\n",
            "        shape: (-1, 1)\n",
            "        name: input_15:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['sequential_1_2/dense_3/Softmax:0'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 43)\n",
            "        name: sequential_1_2/dense_3/Softmax:0\n",
            "  Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fPCFWNhpLEci",
        "colab_type": "code",
        "outputId": "6c562e35-ea68-4aaa-ab0d-2bbe7338f954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ds.label(\"model.directory\", ds.output(\"model\"))\n",
        "ds.label(\"model.framework\", \"tensorflow\")\n",
        "ds.label(\"model.framework.version\", tf.__version__)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "metadata": {
        "id": "f_-g3xy9LEck",
        "colab_type": "code",
        "outputId": "46939b6d-7872-4503-daa0-df35dea17176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "for file in os.listdir(\"model/1\"):\n",
        "    print(file)\n",
        "    ds.output(\"model/1/\" + file)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "variables\n",
            "saved_model.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D1b2oKEgLEcn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds.publish()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}